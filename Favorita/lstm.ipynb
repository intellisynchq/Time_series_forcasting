{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f37df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train.csv\", parse_dates=[\"date\"])\n",
    "df = df.sort_values([\"store_nbr\", \"family\", \"date\"])  # molto importante!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6c8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature temporali\n",
    "df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"day\"] = df[\"date\"].dt.day\n",
    "\n",
    "# Codifica booleana\n",
    "df[\"is_holiday\"] = df[\"is_holiday\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8554d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_store = LabelEncoder()\n",
    "le_family = LabelEncoder()\n",
    "\n",
    "df[\"store_nbr_enc\"] = le_store.fit_transform(df[\"store_nbr\"])\n",
    "df[\"family_enc\"] = le_family.fit_transform(df[\"family\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47fd8a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per ogni serie (store + family) aggiungiamo un contatore temporale\n",
    "df[\"series_id\"] = df[\"store_nbr_enc\"].astype(str) + \"_\" + df[\"family_enc\"].astype(str)\n",
    "df[\"time_idx\"] = df.groupby(\"series_id\").cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e540fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_and_rolling(df):\n",
    "    df = df.sort_values([\"series_id\", \"date\"])\n",
    "\n",
    "    for lag in [1, 7]:\n",
    "        df[f\"lag_{lag}\"] = df.groupby(\"series_id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    for window in [7, 14]:\n",
    "        df[f\"rolling_mean_{window}\"] = (\n",
    "            df.groupby(\"series_id\")[\"sales\"].shift(1).rolling(window=window).mean()\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_lag_and_rolling(df)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e582092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df[\"sales_normalized\"] = 0.0\n",
    "scalers = {}\n",
    "\n",
    "for sid, group in df.groupby(\"series_id\"):\n",
    "    scaler = StandardScaler()\n",
    "    df.loc[group.index, \"sales_normalized\"] = scaler.fit_transform(group[\"sales\"].values.reshape(-1, 1)).flatten()\n",
    "    scalers[sid] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003459d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (2990196, 30, 10)\n",
      "Shape y: (2990196, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(df, input_len=30, forecast_len=7):\n",
    "    X, y, store_ids, family_ids = [], [], [], []\n",
    "\n",
    "    feature_cols = [\n",
    "        \"sales_normalized\", \"onpromotion\", \"is_holiday\", \"dcoilwtico\",\n",
    "        \"dayofweek\", \"month\", \"lag_1\", \"lag_7\", \"rolling_mean_7\", \"rolling_mean_14\"\n",
    "    ]\n",
    "\n",
    "    for sid, group in df.groupby(\"series_id\"):\n",
    "        group = group.sort_values(\"time_idx\")\n",
    "        values = group[feature_cols].values\n",
    "        targets = group[\"sales_normalized\"].values\n",
    "        store = group[\"store_nbr_enc\"].values[0]\n",
    "        family = group[\"family_enc\"].values[0]\n",
    "\n",
    "        for i in range(input_len, len(group) - forecast_len + 1):\n",
    "            X.append(values[i - input_len:i])\n",
    "            y.append(targets[i:i + forecast_len])\n",
    "            store_ids.append(store)\n",
    "            family_ids.append(family)\n",
    "\n",
    "    return (\n",
    "        np.array(X),\n",
    "        np.array(y),\n",
    "        np.array(store_ids),\n",
    "        np.array(family_ids)\n",
    "    )\n",
    "\n",
    "\n",
    "X, y, store_ids, family_ids = create_sequences(df)\n",
    "print(\"Shape X:\", X.shape)  # (n_seq, 30, n_features)\n",
    "print(\"Shape y:\", y.shape)  # (n_seq, forecast_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d77a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SalesDataset(Dataset):\n",
    "    def __init__(self, X, y, store_ids, family_ids):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.store_ids = torch.tensor(store_ids, dtype=torch.long)\n",
    "        self.family_ids = torch.tensor(family_ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.store_ids[idx], self.family_ids[idx]\n",
    "\n",
    "# Split in train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val, store_ids_train, store_ids_val, family_ids_train, family_ids_val = train_test_split(\n",
    "    X, y, store_ids, family_ids, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "train_dataset = SalesDataset(X_train, y_train, store_ids_train, family_ids_train)\n",
    "val_dataset = SalesDataset(X_val, y_val, store_ids_val, family_ids_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6badba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SalesLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, forecast_len=7,\n",
    "                 n_stores=54, n_families=33, emb_dim=8, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.store_emb = nn.Embedding(n_stores, emb_dim)\n",
    "        self.family_emb = nn.Embedding(n_families, emb_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size + 2 * emb_dim, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout_rate)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, forecast_len)\n",
    "\n",
    "    def forward(self, x, store_id, family_id):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        store_e = self.store_emb(store_id).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        family_e = self.family_emb(family_id).unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        x = torch.cat([x, store_e, family_e], dim=-1)  # concat embeddings\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out  # shape: (batch, forecast_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a5585c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.6042, Val Loss: 0.5681\n",
      "Epoch 2 - Train Loss: 0.5653, Val Loss: 0.5661\n",
      "Epoch 3 - Train Loss: 0.5542, Val Loss: 0.5508\n",
      "Epoch 4 - Train Loss: 0.5486, Val Loss: 0.5446\n",
      "Epoch 5 - Train Loss: 0.5436, Val Loss: 0.5434\n",
      "Epoch 6 - Train Loss: 0.5384, Val Loss: 0.5374\n",
      "Epoch 7 - Train Loss: 0.5344, Val Loss: 0.5326\n",
      "Epoch 8 - Train Loss: 0.5319, Val Loss: 0.5319\n",
      "Epoch 9 - Train Loss: 0.5299, Val Loss: 0.5349\n",
      "Epoch 10 - Train Loss: 0.5281, Val Loss: 0.5540\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SalesLSTM(input_size=X.shape[2]).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=20):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch, store_ids, family_ids in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            store_ids, family_ids = store_ids.to(device), family_ids.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch, store_ids, family_ids)\n",
    "            loss = loss_fn(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch, store_ids, family_ids in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                store_ids, family_ids = store_ids.to(device), family_ids.to(device)\n",
    "                preds = model(X_batch, store_ids, family_ids)\n",
    "                loss = loss_fn(preds, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "train(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03631d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sequences(df, input_len=30, forecast_len=7):\n",
    "    X, store_ids, family_ids, series_ids = [], [], [], []\n",
    "\n",
    "    feature_cols = [\n",
    "        \"sales_normalized\", \"onpromotion\", \"is_holiday\", \"dcoilwtico\",\n",
    "        \"dayofweek\", \"month\", \"lag_1\", \"lag_7\", \"rolling_mean_7\", \"rolling_mean_14\"\n",
    "    ]\n",
    "\n",
    "    for sid, group in df.groupby(\"series_id\"):\n",
    "        group = group.sort_values(\"time_idx\")\n",
    "        values = group[feature_cols].values\n",
    "        store = group[\"store_nbr_enc\"].values[0]\n",
    "        family = group[\"family_enc\"].values[0]\n",
    "\n",
    "        for i in range(input_len, len(group) - forecast_len + 1):\n",
    "            X.append(values[i - input_len:i])\n",
    "            store_ids.append(store)\n",
    "            family_ids.append(family)\n",
    "            series_ids.append(sid)\n",
    "\n",
    "    return (\n",
    "        np.array(X),\n",
    "        np.array(store_ids),\n",
    "        np.array(family_ids),\n",
    "        series_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd300325",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (1714,) could not be broadcast to indexing result of shape (1714,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(sales)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(mask):\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mnormed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(sales[mask]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     30\u001b[0m     df_all\u001b[38;5;241m.\u001b[39mloc[group\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m normed\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 5. Ricava solo il test_df aggiornato\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shape mismatch: value array of shape (1714,) could not be broadcast to indexing result of shape (1714,1)"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\", parse_dates=[\"date\"])\n",
    "test_df = test_df.sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "\n",
    "# 1. Unisci train + test per calcolare lag/rolling\n",
    "test_df[\"sales\"] = np.nan  # placeholder per le vendite future\n",
    "df_all = pd.concat([df, test_df], sort=False)\n",
    "\n",
    "# 2. Calcola time_idx e series_id (se non già fatti)\n",
    "df_all[\"store_nbr_enc\"] = le_store.transform(df_all[\"store_nbr\"])\n",
    "df_all[\"family_enc\"] = le_family.transform(df_all[\"family\"])\n",
    "df_all[\"series_id\"] = df_all[\"store_nbr_enc\"].astype(str) + \"_\" + df_all[\"family_enc\"].astype(str)\n",
    "df_all[\"time_idx\"] = df_all.groupby(\"series_id\").cumcount()\n",
    "\n",
    "# 3. Calcola lag e rolling\n",
    "df_all = add_lag_and_rolling(df_all)\n",
    "\n",
    "# 4. Normalizza le vendite dove sono presenti\n",
    "df_all[\"sales_normalized\"] = np.nan\n",
    "\n",
    "for sid, group in df_all.groupby(\"series_id\"):\n",
    "    scaler = scalers[sid]\n",
    "    sales = group[\"sales\"].values.reshape(-1, 1)\n",
    "\n",
    "    normed = np.full_like(sales, np.nan)\n",
    "    mask = ~np.isnan(sales).flatten()\n",
    "\n",
    "    if np.any(mask):\n",
    "        normed[mask] = scaler.transform(sales[mask].reshape(-1, 1)).flatten()\n",
    "\n",
    "    df_all.loc[group.index, \"sales_normalized\"] = normed.flatten()\n",
    "\n",
    "# 5. Ricava solo il test_df aggiornato\n",
    "test_df = df_all[df_all[\"date\"] >= test_df[\"date\"].min()].copy()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _, store_ids, family_ids in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        store_ids = store_ids.to(device)\n",
    "        family_ids = family_ids.to(device)\n",
    "\n",
    "        preds = model(X_batch, store_ids, family_ids)  # (batch, 7)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)  # (n_samples, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_preds = all_preds.reshape(-1, 1)\n",
    "flat_preds_denorm = scaler.inverse_transform(flat_preds)\n",
    "final_preds = flat_preds_denorm.reshape(all_preds.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rows = []\n",
    "for sid, preds in zip(series_ids_test, final_preds):\n",
    "    store, family = sid.split(\"_\")\n",
    "    for i, p in enumerate(preds):\n",
    "        output_rows.append({\n",
    "            \"store_nbr\": int(store),\n",
    "            \"family\": family,\n",
    "            \"day\": i,\n",
    "            \"predicted_sales\": p\n",
    "        })\n",
    "\n",
    "submission_df = pd.DataFrame(output_rows)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
